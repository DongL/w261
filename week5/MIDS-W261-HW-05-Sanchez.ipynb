{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Jason Sanchez*   \n",
    "__Class:__ MIDS w261 (Section *Fall 2016 Group 2*)     \n",
    "__Email:__  *jason.sanchez*@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW5.0](#1.0)   \n",
    "    1.0.  [HW5.1](#1.1)   \n",
    "    1.2.  [HW5.2](#1.2)   \n",
    "    1.3.  [HW5.3](#1.3)    \n",
    "    1.4.  [HW5.4](#1.4)    \n",
    "    1.5.  [HW5.5](#1.5)    \n",
    "    1.5.  [HW5.6](#1.6)    \n",
    "    1.5.  [HW5.7](#1.7)    \n",
    "    1.5.  [HW5.8](#1.8)    \n",
    "    1.5.  [HW5.9](#1.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2016-09-25 \n",
    "\n",
    " === INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "HW4 can be completed locally on your computer\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week\n",
    "\n",
    "<a name=\"3\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.0  <a name=\"1.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data warehouse: Stores a large amount of relational, semi-structured, and unstructured data. Is used for business intelligence and data science.\n",
    "\n",
    "A star schema has fact tables and many dimension tables that connect to the fact tables. Fact tables record events such as sales or website visits and encodes details of the events as keys (user_id, product_id, store_id, ad_id). The dimension tables store the detailed information about each of these keys. \n",
    "\n",
    "Star schemas provide simple approached to structuring data warehouses in a relational way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.1  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3NF means third normal form. It is used to transform large flat files that have repeated data into a linked collection of smaller tables that can be joined on a set of common keys.\n",
    "\n",
    "Machine learning does not use data in 3NF. Instead it uses large flat files so the details that are hidden by the keys can be used in the algorithms.\n",
    "\n",
    "Log files can track specific events of interest. A denormalized log file allows a company to track these events in real time conditioned on specific customer features. Alternatively, a model can be running that triggers appropriate responses based on the next predicted action of a user given the user's latest action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.2  <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List files in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDS-W261-HW-05-Sanchez.ipynb      mostFrequentVisitors.txt\r\n",
      "anonymous-msweb-preprocessed.data\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count lines in dataset. View the first 10 lines. Rename data to log.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   98654 anonymous-msweb-preprocessed.data\n",
      "\n",
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n"
     ]
    }
   ],
   "source": [
    "!wc -l anonymous-msweb-preprocessed.data && echo\n",
    "!head anonymous-msweb-preprocessed.data\n",
    "!cp anonymous-msweb-preprocessed.data log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the output of 4.4 to be just url and url_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     285 urls.txt\n",
      "\n",
      "\"/regwiz\",1000\n",
      "\"/support\",1001\n",
      "\"/athome\",1002\n",
      "\"/kb\",1003\n",
      "\"/search\",1004\n",
      "\"/norge\",1005\n",
      "\"/misc\",1006\n",
      "\"/ie_intl\",1007\n",
      "\"/msdownload\",1008\n",
      "\"/windows\",1009\n"
     ]
    }
   ],
   "source": [
    "!cat mostFrequentVisitors.txt | cut -f 1,2 -d',' > urls.txt\n",
    "!wc -l urls.txt && echo\n",
    "!head urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls.txt file is much smaller than the log.txt data and should be what is loaded into memory. This means it would be the right-side table in a left join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left-side join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lj.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "# Avoid broken pipe error\n",
    "from signal import signal, SIGPIPE, SIG_DFL\n",
    "signal(SIGPIPE,SIG_DFL) \n",
    "\n",
    "class LJ(MRJob):\n",
    "    def mapper_init(self):\n",
    "        self.urls = {}\n",
    "        with open(\"urls.txt\") as urls:\n",
    "            for line in urls:\n",
    "                url, key = line.strip().replace('\"',\"\").split(\",\")\n",
    "                self.urls[key] = url\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        try:\n",
    "            yield (lines, self.urls[lines[2:6]])\n",
    "        except ValueError:\n",
    "            yield (lines, \"\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    LJ.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"V,1000,1,C,10001\"\t\"/regwiz\"\r\n",
      "\"V,1001,1,C,10001\"\t\"/support\"\r\n",
      "\"V,1002,1,C,10001\"\t\"/athome\"\r\n",
      "\"V,1001,1,C,10002\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10002\"\t\"/kb\"\r\n",
      "\"V,1001,1,C,10003\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10003\"\t\"/kb\"\r\n",
      "\"V,1004,1,C,10003\"\t\"/search\"\r\n",
      "\"V,1005,1,C,10004\"\t\"/norge\"\r\n",
      "\"V,1006,1,C,10005\"\t\"/misc\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat log.txt | python lj.py --file urls.txt -q | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ij.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ij.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "# Avoid broken pipe error\n",
    "from signal import signal, SIGPIPE, SIG_DFL\n",
    "signal(SIGPIPE,SIG_DFL) \n",
    "\n",
    "class IJ(MRJob):\n",
    "    def mapper_init(self):\n",
    "        self.urls = {}\n",
    "        with open(\"urls.txt\") as urls:\n",
    "            for line in urls:\n",
    "                url, key = line.strip().replace('\"',\"\").split(\",\")\n",
    "                self.urls[key] = url\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        try:\n",
    "            yield (lines, self.urls[lines[2:6]])\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    IJ.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"V,1000,1,C,10001\"\t\"/regwiz\"\r\n",
      "\"V,1001,1,C,10001\"\t\"/support\"\r\n",
      "\"V,1002,1,C,10001\"\t\"/athome\"\r\n",
      "\"V,1001,1,C,10002\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10002\"\t\"/kb\"\r\n",
      "\"V,1001,1,C,10003\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10003\"\t\"/kb\"\r\n",
      "\"V,1004,1,C,10003\"\t\"/search\"\r\n",
      "\"V,1005,1,C,10004\"\t\"/norge\"\r\n",
      "\"V,1006,1,C,10005\"\t\"/misc\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat log.txt | python ij.py --file urls.txt -q | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rj.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "# Avoid broken pipe error\n",
    "from signal import signal, SIGPIPE, SIG_DFL\n",
    "signal(SIGPIPE,SIG_DFL) \n",
    "\n",
    "class RJ(MRJob):\n",
    "    def mapper_init(self):\n",
    "        self.urls_used = set()\n",
    "        self.urls = {}\n",
    "        with open(\"urls.txt\") as urls:\n",
    "            for line in urls:\n",
    "                url, key = line.strip().replace('\"',\"\").split(\",\")\n",
    "                self.urls[key] = url\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        try:\n",
    "            url = lines[2:6]\n",
    "            yield (self.urls[url], lines)\n",
    "            self.urls_used.add(url)\n",
    "            \n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        for key, value in self.urls.items():\n",
    "            if key not in self.urls_used:\n",
    "                yield (self.urls[key], \"*\")\n",
    "                \n",
    "    \n",
    "                \n",
    "    def reducer(self, url, values):\n",
    "        quick_stash = 0\n",
    "        for val in values:\n",
    "            if val != \"*\":\n",
    "                quick_stash += 1\n",
    "                yield (url, val)\n",
    "        if quick_stash == 0:\n",
    "            yield (url, \"None\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    RJ.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove it works, we can only use the first 100 log entries. We see that urls without corresponding log entries are listed as \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/access\"\t\"None\"\r\n",
      "\"/accessdev\"\t\"None\"\r\n",
      "\"/activeplatform\"\t\"None\"\r\n",
      "\"/activex\"\t\"None\"\r\n",
      "\"/adc\"\t\"None\"\r\n",
      "\"/ado\"\t\"None\"\r\n",
      "\"/ads\"\t\"None\"\r\n",
      "\"/advtech\"\t\"None\"\r\n",
      "\"/argentina\"\t\"None\"\r\n",
      "\"/atec\"\t\"None\"\r\n",
      "\"/athome\"\t\"V,1002,1,C,10001\"\r\n",
      "\"/athome\"\t\"V,1002,1,C,10019\"\r\n",
      "\"/athome\"\t\"V,1002,1,C,10020\"\r\n",
      "\"/athome\"\t\"V,1002,1,C,10031\"\r\n",
      "\"/australia\"\t\"None\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 log.txt | python rj.py --file urls.txt -q | head -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3 <a name=\"1.3\"></a> Systems tests on n-grams dataset (Phase1) and full experiment (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "## 3.  HW5.3.0 Run Systems tests locally (PHASE1)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. We should calculate the stripes cooccurrence data from the raw text and not from the 5-gram preprocessed data. Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 5gram_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 5gram_test.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to ingest data for large corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting GetIndexandOtherWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile GetIndexandOtherWords.py\n",
    "\n",
    "import heapq\n",
    "from re import findall\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class TopList(list):\n",
    "    def __init__(self, max_size, num_position=0):\n",
    "        \"\"\"\n",
    "        Just like a list, except the append method adds the new value to the \n",
    "        list only if it is larger than the smallest value (or if the size of \n",
    "        the list is less than max_size). \n",
    "        \n",
    "        If each element of the list is an int or float, uses that value for \n",
    "        comparison. If the elements in the list are lists or tuples, uses the \n",
    "        list_position element of the list or tuple for the comparison.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pos = num_position\n",
    "        \n",
    "    def _get_key(self, x):\n",
    "        return x[self.pos] if isinstance(x, (list, tuple)) else x\n",
    "        \n",
    "    def append(self, val):\n",
    "        if len(self) < self.max_size:\n",
    "            heapq.heappush(self, val)\n",
    "        elif self._get_key(self[0]) < self._get_key(val):\n",
    "            heapq.heapreplace(self, val)\n",
    "            \n",
    "    def final_sort(self):\n",
    "        return sorted(self, key=self._get_key, reverse=True)\n",
    "\n",
    "    \n",
    "class GetIndexandOtherWords(MRJob):\n",
    "    \"\"\"\n",
    "    Usage: python GetIndexandOtherWords.py --index-range 9000-10000 --top-n-words 10000 --use-term-counts True\n",
    "    \n",
    "    Given n-gram formatted data, outputs a file of the form:\n",
    "    \n",
    "    index    term\n",
    "    index    term\n",
    "    ...\n",
    "    word     term\n",
    "    word     term\n",
    "    ...\n",
    "    \n",
    "    Where there would be 1001 index words and 10000 total words. Each word would be ranked based\n",
    "    on either the term count listed in the Google n-gram data (i.e. the counts found in the\n",
    "    underlying books) or the ranks would be based on the word count of the n-grams in the actual\n",
    "    dataset (i.e. ignore the numbers/counts associated with each n-gram and count each n-gram\n",
    "    exactly once).\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(GetIndexandOtherWords, self).configure_options()\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--index-range', \n",
    "            default=\"9-10\", \n",
    "            help=\"Specify the range of the index words. ex. 9-10 means the ninth and \" +\n",
    "                 \"tenth most popular words will serve as the index\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--top-n-words', \n",
    "            default=\"10\", \n",
    "            help=\"Specify the number of words to output in all\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--use-term-counts', \n",
    "            default=\"True\", \n",
    "            choices=[\"True\",\"False\"],\n",
    "            help=\"When calculating the most frequent words, choose whether to count \" + \n",
    "            \"each word based on the term counts reported by Google or just based on \" + \n",
    "            \"the number of times the word appears in an n-gram\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--return-counts', \n",
    "            default=\"False\", \n",
    "            choices=[\"True\",\"False\"],\n",
    "            help=\"The final output includes the counts of each word\")\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Ensure command line options are sane\n",
    "        top_n_words = int(self.options.top_n_words)\n",
    "        last_index_word = int(self.options.index_range.split(\"-\")[1])\n",
    "        if top_n_words < last_index_word:\n",
    "            raise ValueError(\"\"\"--top-n-words value (currently %d) must be equal to or greater than\n",
    "                             --index-range value (currently %d).\"\"\" % (top_n_words, last_index_word))\n",
    "        \n",
    "        self.stop_words =  {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "                            'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "                            'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', \n",
    "                            'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                            'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "                            'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "                            'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "                            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "                            'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                            'with', 'about', 'against', 'between', 'into', 'through', \n",
    "                            'during', 'before', 'after', 'above', 'below', 'to', 'from', \n",
    "                            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "                            'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                            'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                            'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "                            'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n",
    "                            'can', 'will', 'just', 'don', 'should', 'now'}\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        \n",
    "        # Either use the ngram term count for the count or count each word just once\n",
    "        if self.options.use_term_counts == \"True\":\n",
    "            term_count = int(term_count)\n",
    "        else:\n",
    "            term_count = 1\n",
    "        \n",
    "        # Iterate through each term. Skip stop words\n",
    "        for term in findall(r'[a-z]+', terms.lower()):\n",
    "            if term in self.stop_words:\n",
    "                pass\n",
    "            else:\n",
    "                yield (term, term_count)\n",
    "        \n",
    "    def combiner(self, term, counts):\n",
    "        yield (term, sum(counts))      \n",
    "        \n",
    "        \n",
    "    def reducer_init(self):\n",
    "        \"\"\"\n",
    "        Accumulates the top X words and yields them. Note: should only use if\n",
    "        you want to emit a reasonable amount of top words (i.e. an amount that \n",
    "        could fit on a single computer.)\n",
    "        \"\"\"\n",
    "        self.top_n_words = int(self.options.top_n_words)\n",
    "        self.TopTerms = TopList(self.top_n_words, num_position=1)\n",
    "        \n",
    "    def reducer(self, term, counts):\n",
    "        self.TopTerms.append((term, sum(counts)))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for pair in self.TopTerms:\n",
    "            yield pair\n",
    "        \n",
    "    def mapper_single_key(self, term, count):\n",
    "        \"\"\"\n",
    "        Send all the data to a single reducer\n",
    "        \"\"\"\n",
    "        yield (1, (term, count))\n",
    "        \n",
    "    def reducer_init_top_vals(self):\n",
    "        # Collect top words\n",
    "        self.top_n_words = int(self.options.top_n_words)\n",
    "        self.TopTerms = TopList(self.top_n_words, num_position=1)\n",
    "        # Collect index words\n",
    "        self.index_range = [int(num) for num in self.options.index_range.split(\"-\")]\n",
    "        self.index_low, self.index_high = self.index_range\n",
    "        # Control if output shows counts or just words\n",
    "        self.return_counts = self.options.return_counts == \"True\"\n",
    "\n",
    "    def reducer_top_vals(self, _, terms):\n",
    "        for term in terms:\n",
    "            self.TopTerms.append(term)\n",
    "            \n",
    "    def reducer_final_top_vals(self):\n",
    "        TopTerms = self.TopTerms.final_sort()\n",
    "        \n",
    "        if self.return_counts:            \n",
    "            # Yield index words\n",
    "            for term in TopTerms[self.index_low-1:self.index_high]:\n",
    "                yield (\"index\", term)\n",
    "\n",
    "            # Yield all words\n",
    "            for term in TopTerms:\n",
    "                yield (\"words\", term)\n",
    "        else:\n",
    "            # Yield index words\n",
    "            for term in TopTerms[self.index_low-1:self.index_high]:\n",
    "                yield (\"index\", term[0])\n",
    "\n",
    "            # Yield all words\n",
    "            for term in TopTerms:\n",
    "                yield (\"words\", term[0])\n",
    "    \n",
    "    def steps(self):\n",
    "        \"\"\"\n",
    "        Step one: Yield top n-words from each reducer. Means dataset size is \n",
    "                  n-words * num_reducers. Guarantees overall top n-words are \n",
    "                  sent to the next step.\n",
    "        \n",
    "        \"\"\"\n",
    "        mr_steps = [MRStep(mapper_init=self.mapper_init,\n",
    "                           mapper=self.mapper,\n",
    "                           combiner=self.combiner,\n",
    "                           reducer_init=self.reducer_init,\n",
    "                           reducer_final=self.reducer_final,\n",
    "                           reducer=self.reducer),\n",
    "                    MRStep(mapper=self.mapper_single_key,\n",
    "                           reducer_init=self.reducer_init_top_vals,\n",
    "                           reducer=self.reducer_top_vals,\n",
    "                           reducer_final=self.reducer_final_top_vals)]\n",
    "        return mr_steps\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    GetIndexandOtherWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t\"circumstantial\"\r\n",
      "\"index\"\t\"establishing\"\r\n",
      "\"words\"\t\"child\"\r\n",
      "\"words\"\t\"christmas\"\r\n",
      "\"words\"\t\"wales\"\r\n",
      "\"words\"\t\"female\"\r\n",
      "\"words\"\t\"collection\"\r\n",
      "\"words\"\t\"fairy\"\r\n",
      "\"words\"\t\"government\"\r\n",
      "\"words\"\t\"city\"\r\n",
      "\"words\"\t\"circumstantial\"\r\n",
      "\"words\"\t\"establishing\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat 5gram_test.txt | python GetIndexandOtherWords.py --index-range 9-10     \\\n",
    "                                                      --top-n-words 10       \\\n",
    "                                                      --return-counts False  \\\n",
    "                                                       -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For _HW 5.4-5.5_,  unit test and regression test your code using the  followings small test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see above]\n",
    "* stripe-docs-test [see below]\n",
    "* atlas-boon-test [see below]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test\n",
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Stripes for systems test 1 (predefined)\n",
    "############################################\n",
    "\n",
    "with open(\"mini_stripes.txt\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat mini_stripes.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Phase 1\n",
    "Complete 5.4 and 5.5 and systems test them using the above test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the following results (for stripes, inverted index, and pairwise similarities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MakeStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MakeStripes.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "\n",
    "class MakeStripes(MRJob):\n",
    "    def mapper(self, _, lines):\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        terms = terms.split()\n",
    "        term_count = int(term_count)\n",
    "        \n",
    "        for item in terms:\n",
    "            yield (item, {term:term_count for term in terms if term != item})\n",
    "        \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MakeStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\r\n",
      "\"boon\"\t{\"dipped\": 10, \"cava\": 10, \"atlas\": 50}\r\n",
      "\"cava\"\t{\"boon\": 10, \"dipped\": 10}\r\n",
      "\"dipped\"\t{\"boon\": 10, \"atlas\": 15, \"cava\": 10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas-boon-systems-test.txt | python MakeStripes.py -r local -q > atlas_stripes.txt\n",
    "!cat atlas_stripes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stripe documents for  atlas-boon systems test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\n",
    "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\n",
    "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\n",
    "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated stripes match the systems test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InvertIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InvertIndex.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from collections import Counter\n",
    "\n",
    "class InvertIndex(MRJob):\n",
    "    MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key, words):\n",
    "        n_words = len(words)\n",
    "        \n",
    "        for word in words: \n",
    "            yield (word, {key:n_words})\n",
    "            \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    InvertIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"boon\": 3, \"dipped\": 3}\r\n",
      "\"boon\"\t{\"atlas\": 2, \"cava\": 2, \"dipped\": 3}\r\n",
      "\"cava\"\t{\"boon\": 3, \"dipped\": 3}\r\n",
      "\"dipped\"\t{\"cava\": 2, \"atlas\": 2, \"boon\": 3}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas_stripes.txt | python InvertIndex.py -q > atlas_inverted.txt\n",
    "!cat atlas_inverted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"M\"\t{\"DocC\": 4}\r\n",
      "\"N\"\t{\"DocC\": 4}\r\n",
      "\"X\"\t{\"DocB\": 2, \"DocA\": 3}\r\n",
      "\"Y\"\t{\"DocB\": 2, \"DocA\": 3, \"DocC\": 4}\r\n",
      "\"Z\"\t{\"DocA\": 3, \"DocC\": 4}\r\n"
     ]
    }
   ],
   "source": [
    "!cat mini_stripes.txt | python InvertIndex.py -q > mini_stripes_inverted.txt\n",
    "!cat mini_stripes_inverted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Systems test mini_stripes - Inverted Index\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "            \"M\" |         DocC 4 |                |               \n",
    "            \"N\" |         DocC 4 |                |               \n",
    "            \"X\" |         DocA 3 |         DocB 2 |               \n",
    "            \"Y\" |         DocA 3 |         DocB 2 |         DocC 4\n",
    "            \"Z\" |         DocA 3 |         DocC 4 |               \n",
    "\n",
    " systems test atlas-boon - Inverted Index\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "        \"atlas\" |         boon 3 |       dipped 3 |               \n",
    "       \"dipped\" |        atlas 2 |         boon 3 |         cava 2\n",
    "         \"boon\" |        atlas 2 |         cava 2 |       dipped 3\n",
    "         \"cava\" |         boon 3 |       dipped 3 |        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from itertools import combinations\n",
    "from statistics import mean\n",
    "\n",
    "class Similarity(MRJob):\n",
    "    MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key_term, docs):\n",
    "        doc_names = docs.keys()\n",
    "        for doc_pairs in combinations(sorted(list(doc_names)), 2):\n",
    "            yield (doc_pairs, 1)\n",
    "        for name in doc_names:\n",
    "            yield (name, 1)\n",
    "            \n",
    "    def combiner(self, key, value):\n",
    "        yield (key, sum(value))\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.words = {}\n",
    "        self.results = []\n",
    "    \n",
    "    def reducer(self, doc_or_docs, count):\n",
    "        if isinstance(doc_or_docs, str):\n",
    "            self.words[doc_or_docs] = sum(count)\n",
    "        else:\n",
    "            d1, d2 = doc_or_docs\n",
    "            d1_n_words, d2_n_words = self.words[d1], self.words[d2]\n",
    "            intersection = sum(count)\n",
    "            \n",
    "            jaccard = round(intersection/(d1_n_words + d2_n_words - intersection), 3)\n",
    "            cosine = round(intersection/(d1_n_words**.5 * d2_n_words**.5), 3)\n",
    "            dice = round(2*intersection/(d1_n_words + d2_n_words), 3)\n",
    "            overlap = round(intersection/min(d1_n_words, d2_n_words), 3)\n",
    "            average = round(mean([jaccard, cosine, dice, overlap]), 3)\n",
    "            \n",
    "            self.results.append([doc_or_docs, {\"jacc\":jaccard, \"cos\":cosine, \n",
    "                                               \"dice\":dice, \"ol\":overlap, \"ave\":average}])\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        for doc, result in sorted(self.results, key=lambda x: x[1][\"ave\"], reverse=True):\n",
    "            yield (doc, result)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    Similarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas\", \"cava\"]\t{\"ave\": 1.0, \"cos\": 1.0, \"ol\": 1.0, \"dice\": 1.0, \"jacc\": 1.0}\r\n",
      "[\"boon\", \"dipped\"]\t{\"ave\": 0.625, \"cos\": 0.667, \"ol\": 0.667, \"dice\": 0.667, \"jacc\": 0.5}\r\n",
      "[\"atlas\", \"boon\"]\t{\"ave\": 0.39, \"cos\": 0.408, \"ol\": 0.5, \"dice\": 0.4, \"jacc\": 0.25}\r\n",
      "[\"atlas\", \"dipped\"]\t{\"ave\": 0.39, \"cos\": 0.408, \"ol\": 0.5, \"dice\": 0.4, \"jacc\": 0.25}\r\n",
      "[\"boon\", \"cava\"]\t{\"ave\": 0.39, \"cos\": 0.408, \"ol\": 0.5, \"dice\": 0.4, \"jacc\": 0.25}\r\n",
      "[\"cava\", \"dipped\"]\t{\"ave\": 0.39, \"cos\": 0.408, \"ol\": 0.5, \"dice\": 0.4, \"jacc\": 0.25}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas_inverted.txt | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\", \"DocB\"]\t{\"cos\": 0.816, \"ave\": 0.821, \"dice\": 0.8, \"jacc\": 0.667, \"ol\": 1.0}\r\n",
      "[\"DocA\", \"DocC\"]\t{\"cos\": 0.577, \"ave\": 0.554, \"dice\": 0.571, \"jacc\": 0.4, \"ol\": 0.667}\r\n",
      "[\"DocB\", \"DocC\"]\t{\"cos\": 0.354, \"ave\": 0.347, \"dice\": 0.333, \"jacc\": 0.2, \"ol\": 0.5}\r\n"
     ]
    }
   ],
   "source": [
    "!cat mini_stripes_inverted.txt | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Systems test mini_stripes - Similarity measures\n",
    "\n",
    "| average |           pair |         cosine |        jaccard |        overlap |           dice |\n",
    "|-|-|-|-|-|-|\n",
    "|       0.741582 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000 |\n",
    "|       0.488675 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429 |\n",
    "|       0.276777 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Systems test atlas-boon 2 - Similarity measures\n",
    "\n",
    "| average |           pair |         cosine |        jaccard |        overlap |           dice |\n",
    "|-|-|-|-|-|-|       \n",
    "|1.000000 |   atlas - cava |       1.000000 |       1.000000 |        1.000000 |       1.000000|\n",
    "|       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667|\n",
    "|       0.389562 |  cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000|\n",
    "|       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000|\n",
    "|       0.389562 | atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000|\n",
    "|       0.389562 |   atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers I calculated exactly match the systems test except for the average calculations of the mini_stripes set. In this instance, the systems test calculations are not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code on AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Using s3://mrjob-3d3e189cec521ef3/tmp/ as our temp dir on S3\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/MakeStripes.Jason.20161003.090134.937573\n",
      "Copying local files to s3://mrjob-3d3e189cec521ef3/tmp/MakeStripes.Jason.20161003.090134.937573/files/...\n",
      "Created new cluster j-5ZRHGZTUSQSW\n",
      "Waiting for step 1 of 1 (s-25AWSL4OHYR4F) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  RUNNING for 31.3s\n",
      "  RUNNING for 62.9s\n",
      "  RUNNING for 93.7s\n",
      "  RUNNING for 125.0s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Waiting for cluster (j-5ZRHGZTUSQSW) to terminate...\n",
      "  TERMINATING\n",
      "  TERMINATING\n",
      "  TERMINATED\n",
      "Looking for step log in s3://mrjob-3d3e189cec521ef3/tmp/logs/j-5ZRHGZTUSQSW/steps/s-25AWSL4OHYR4F...\n",
      "  Parsing step log: s3://mrjob-3d3e189cec521ef3/tmp/logs/j-5ZRHGZTUSQSW/steps/s-25AWSL4OHYR4F/syslog.gz\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=163\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=151\n",
      "\t\tFILE: Number of bytes written=312061\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=316\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=2\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=101\n",
      "\t\tS3: Number of bytes written=163\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=26241024\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=21355520\n",
      "\t\tTotal time spent by all map tasks (ms)=34168\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=102504\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20855\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=83420\n",
      "\t\tTotal vcore-seconds taken by all map tasks=34168\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=20855\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3500\n",
      "\t\tCombine input records=7\n",
      "\t\tCombine output records=6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1229\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=190\n",
      "\t\tMap output materialized bytes=184\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=906825728\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=184\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=598155264\n",
      "\t\tVirtual memory (bytes) snapshot=3938279424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from s3://mrjob-3d3e189cec521ef3/tmp/MakeStripes.Jason.20161003.090134.937573/output/...\n",
      "\"atlas\"\t{\"boon\": 50, \"dipped\": 15}\n",
      "\"boon\"\t{\"cava\": 10, \"atlas\": 50, \"dipped\": 10}\n",
      "\"cava\"\t{\"boon\": 10, \"dipped\": 10}\n",
      "\"dipped\"\t{\"cava\": 10, \"boon\": 10, \"atlas\": 15}\n",
      "Removing s3 temp directory s3://mrjob-3d3e189cec521ef3/tmp/MakeStripes.Jason.20161003.090134.937573/...\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/MakeStripes.Jason.20161003.090134.937573...\n",
      "Removing log files in s3://mrjob-3d3e189cec521ef3/tmp/logs/j-5ZRHGZTUSQSW/...\n",
      "Terminating cluster: j-5ZRHGZTUSQSW\n",
      "CPU times: user 8.31 s, sys: 2.53 s, total: 10.8 s\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!source ../private/aws_creds.sh && python MakeStripes.py -r emr atlas-boon-systems-test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2: Full-scale experiment on Google N-gram data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.2  Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.4 OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4  <a name=\"1.4\"></a> Synonym detection over 2Gig of Data\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"\\nTop/Bottom 20 results - Similarity measures - sorted by cosine\"\n",
    "print \"(From the entire data set)\"\n",
    "print '—'*117\n",
    "print \"{0:>30} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\", \"average\")\n",
    "print '-'*117\n",
    "\n",
    "for stripe in sortedSims[:20]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n",
    "\n",
    "print '—'*117\n",
    "\n",
    "for stripe in sortedSims[-20:]:\n",
    "    print \"{0:>30} |{1:>15f} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "        stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]), float(stripe[5]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"1.5\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "## For this part we can use one of three outputs. They are all the same, but were generated differently\n",
    "# 1. the top 1000 from the full sorted dataset -> sortedSims[:1000]\n",
    "# 2. the top 1000 from the partial sort aggragate file -> sims2/top1000sims\n",
    "# 3. the top 1000 from the total order sort file -> head -1000 sims_parts/part-00004\n",
    "\n",
    "top1000sims = []\n",
    "with open(\"sims2/top1000sims\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "\n",
    "        line = line.strip()\n",
    "        avg,lisst = line.split(\"\\t\")\n",
    "        lisst = json.loads(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    pair = line[0]\n",
    "    words = pair.split(\" - \")\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"1.6\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"1.7\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8 <a name=\"1.8\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"1.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

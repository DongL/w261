{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 5gram_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile 5gram_test.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to ingest data for large corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting GetIndexandOtherWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile GetIndexandOtherWords.py\n",
    "\n",
    "import heapq\n",
    "from re import findall\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class TopList(list):\n",
    "    def __init__(self, max_size, num_position=0):\n",
    "        \"\"\"\n",
    "        Just like a list, except the append method adds the new value to the \n",
    "        list only if it is larger than the smallest value (or if the size of \n",
    "        the list is less than max_size). \n",
    "        \n",
    "        If each element of the list is an int or float, uses that value for \n",
    "        comparison. If the elements in the list are lists or tuples, uses the \n",
    "        list_position element of the list or tuple for the comparison.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pos = num_position\n",
    "        \n",
    "    def _get_key(self, x):\n",
    "        return x[self.pos] if isinstance(x, (list, tuple)) else x\n",
    "        \n",
    "    def append(self, val):\n",
    "        if len(self) < self.max_size:\n",
    "            heapq.heappush(self, val)\n",
    "        elif self._get_key(self[0]) < self._get_key(val):\n",
    "            heapq.heapreplace(self, val)\n",
    "            \n",
    "    def final_sort(self):\n",
    "        return sorted(self, key=self._get_key, reverse=True)\n",
    "\n",
    "    \n",
    "class GetIndexandOtherWords(MRJob):\n",
    "    \"\"\"\n",
    "    Usage: python GetIndexandOtherWords.py --index-range 9000-10000 --top-n-words 10000 --use-term-counts True\n",
    "    \n",
    "    Given n-gram formatted data, outputs a file of the form:\n",
    "    \n",
    "    index    term\n",
    "    index    term\n",
    "    ...\n",
    "    word     term\n",
    "    word     term\n",
    "    ...\n",
    "    \n",
    "    Where there would be 1001 index words and 10000 total words. Each word would be ranked based\n",
    "    on either the term count listed in the Google n-gram data (i.e. the counts found in the\n",
    "    underlying books) or the ranks would be based on the word count of the n-grams in the actual\n",
    "    dataset (i.e. ignore the numbers/counts associated with each n-gram and count each n-gram\n",
    "    exactly once).\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(GetIndexandOtherWords, self).configure_options()\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--index-range', \n",
    "            default=\"9-10\", \n",
    "            help=\"Specify the range of the index words. ex. 9-10 means the ninth and \" +\n",
    "                 \"tenth most popular words will serve as the index\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--top-n-words', \n",
    "            default=\"10\", \n",
    "            help=\"Specify the number of words to output in all\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--use-term-counts', \n",
    "            default=\"True\", \n",
    "            choices=[\"True\",\"False\"],\n",
    "            help=\"When calculating the most frequent words, choose whether to count \" + \n",
    "            \"each word based on the term counts reported by Google or just based on \" + \n",
    "            \"the number of times the word appears in an n-gram\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--return-counts', \n",
    "            default=\"False\", \n",
    "            choices=[\"True\",\"False\"],\n",
    "            help=\"The final output includes the counts of each word\")\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Ensure command line options are sane\n",
    "        top_n_words = int(self.options.top_n_words)\n",
    "        last_index_word = int(self.options.index_range.split(\"-\")[1])\n",
    "        if top_n_words < last_index_word:\n",
    "            raise ValueError(\"\"\"--top-n-words value (currently %d) must be equal to or greater than\n",
    "                             --index-range value (currently %d).\"\"\" % (top_n_words, last_index_word))\n",
    "        \n",
    "        self.stop_words =  {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "                            'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "                            'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', \n",
    "                            'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                            'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "                            'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "                            'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "                            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "                            'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                            'with', 'about', 'against', 'between', 'into', 'through', \n",
    "                            'during', 'before', 'after', 'above', 'below', 'to', 'from', \n",
    "                            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "                            'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                            'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                            'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "                            'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n",
    "                            'can', 'will', 'just', 'don', 'should', 'now'}\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        \n",
    "        # Either use the ngram term count for the count or count each word just once\n",
    "        if self.options.use_term_counts == \"True\":\n",
    "            term_count = int(term_count)\n",
    "        else:\n",
    "            term_count = 1\n",
    "        \n",
    "        # Iterate through each term. Skip stop words\n",
    "        for term in findall(r'[a-z]+', terms.lower()):\n",
    "            if term in self.stop_words:\n",
    "                pass\n",
    "            else:\n",
    "                yield (term, term_count)\n",
    "        \n",
    "    def combiner(self, term, counts):\n",
    "        yield (term, sum(counts))      \n",
    "        \n",
    "        \n",
    "    def reducer_init(self):\n",
    "        \"\"\"\n",
    "        Accumulates the top X words and yields them. Note: should only use if\n",
    "        you want to emit a reasonable amount of top words (i.e. an amount that \n",
    "        could fit on a single computer.)\n",
    "        \"\"\"\n",
    "        self.top_n_words = int(self.options.top_n_words)\n",
    "        self.TopTerms = TopList(self.top_n_words, num_position=1)\n",
    "        \n",
    "    def reducer(self, term, counts):\n",
    "        self.TopTerms.append((term, sum(counts)))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for pair in self.TopTerms:\n",
    "            yield pair\n",
    "        \n",
    "    def mapper_single_key(self, term, count):\n",
    "        \"\"\"\n",
    "        Send all the data to a single reducer\n",
    "        \"\"\"\n",
    "        yield (1, (term, count))\n",
    "        \n",
    "    def reducer_init_top_vals(self):\n",
    "        # Collect top words\n",
    "        self.top_n_words = int(self.options.top_n_words)\n",
    "        self.TopTerms = TopList(self.top_n_words, num_position=1)\n",
    "        # Collect index words\n",
    "        self.index_range = [int(num) for num in self.options.index_range.split(\"-\")]\n",
    "        self.index_low, self.index_high = self.index_range\n",
    "        # Control if output shows counts or just words\n",
    "        self.return_counts = self.options.return_counts == \"True\"\n",
    "\n",
    "    def reducer_top_vals(self, _, terms):\n",
    "        for term in terms:\n",
    "            self.TopTerms.append(term)\n",
    "            \n",
    "    def reducer_final_top_vals(self):\n",
    "        TopTerms = self.TopTerms.final_sort()\n",
    "        \n",
    "        if self.return_counts:            \n",
    "            # Yield index words\n",
    "            for term in TopTerms[self.index_low-1:self.index_high]:\n",
    "                yield (\"index\", term)\n",
    "\n",
    "            # Yield all words\n",
    "            for term in TopTerms:\n",
    "                yield (\"words\", term)\n",
    "        else:\n",
    "            # Yield index words\n",
    "            for term in TopTerms[self.index_low-1:self.index_high]:\n",
    "                yield (\"index\", term[0])\n",
    "\n",
    "            # Yield all words\n",
    "            for term in TopTerms:\n",
    "                yield (\"words\", term[0])\n",
    "    \n",
    "    def steps(self):\n",
    "        \"\"\"\n",
    "        Step one: Yield top n-words from each reducer. Means dataset size is \n",
    "                  n-words * num_reducers. Guarantees overall top n-words are \n",
    "                  sent to the next step.\n",
    "        \n",
    "        \"\"\"\n",
    "        mr_steps = [MRStep(mapper_init=self.mapper_init,\n",
    "                           mapper=self.mapper,\n",
    "                           combiner=self.combiner,\n",
    "                           reducer_init=self.reducer_init,\n",
    "                           reducer_final=self.reducer_final,\n",
    "                           reducer=self.reducer),\n",
    "                    MRStep(mapper=self.mapper_single_key,\n",
    "                           reducer_init=self.reducer_init_top_vals,\n",
    "                           reducer=self.reducer_top_vals,\n",
    "                           reducer_final=self.reducer_final_top_vals)]\n",
    "        return mr_steps\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    GetIndexandOtherWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t\"circumstantial\"\r\n",
      "\"index\"\t\"establishing\"\r\n",
      "\"words\"\t\"child\"\r\n",
      "\"words\"\t\"christmas\"\r\n",
      "\"words\"\t\"wales\"\r\n",
      "\"words\"\t\"female\"\r\n",
      "\"words\"\t\"collection\"\r\n",
      "\"words\"\t\"fairy\"\r\n",
      "\"words\"\t\"government\"\r\n",
      "\"words\"\t\"city\"\r\n",
      "\"words\"\t\"circumstantial\"\r\n",
      "\"words\"\t\"establishing\"\r\n"
     ]
    }
   ],
   "source": [
    "# Very close to what we will run on the full dataset. Returns whether the word is an index \n",
    "# term or just a normal word.\n",
    "!cat 5gram_test.txt | python GetIndexandOtherWords.py --index-range 9-10     \\\n",
    "                                                      --top-n-words 10       \\\n",
    "                                                       -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t[\"circumstantial\", 62]\r\n",
      "\"index\"\t[\"establishing\", 59]\r\n",
      "\"words\"\t[\"child\", 1099]\r\n",
      "\"words\"\t[\"christmas\", 1099]\r\n",
      "\"words\"\t[\"wales\", 1099]\r\n",
      "\"words\"\t[\"female\", 447]\r\n",
      "\"words\"\t[\"collection\", 239]\r\n",
      "\"words\"\t[\"fairy\", 123]\r\n",
      "\"words\"\t[\"government\", 102]\r\n",
      "\"words\"\t[\"city\", 62]\r\n",
      "\"words\"\t[\"circumstantial\", 62]\r\n",
      "\"words\"\t[\"establishing\", 59]\r\n"
     ]
    }
   ],
   "source": [
    "# Returns the counts of each word as well\n",
    "!cat 5gram_test.txt | python GetIndexandOtherWords.py --index-range 9-10     \\\n",
    "                                                      --top-n-words 10       \\\n",
    "                                                      --return-counts True   \\\n",
    "                                                       -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t[\"collection\", 239]\r\n",
      "\"index\"\t[\"fairy\", 123]\r\n",
      "\"index\"\t[\"government\", 102]\r\n",
      "\"index\"\t[\"city\", 62]\r\n",
      "\"index\"\t[\"circumstantial\", 62]\r\n",
      "\"index\"\t[\"establishing\", 59]\r\n",
      "\"words\"\t[\"child\", 1099]\r\n",
      "\"words\"\t[\"christmas\", 1099]\r\n",
      "\"words\"\t[\"wales\", 1099]\r\n",
      "\"words\"\t[\"female\", 447]\r\n",
      "\"words\"\t[\"collection\", 239]\r\n",
      "\"words\"\t[\"fairy\", 123]\r\n",
      "\"words\"\t[\"government\", 102]\r\n",
      "\"words\"\t[\"city\", 62]\r\n",
      "\"words\"\t[\"circumstantial\", 62]\r\n",
      "\"words\"\t[\"establishing\", 59]\r\n"
     ]
    }
   ],
   "source": [
    "# Shows how to adjust the index_range argument\n",
    "!cat 5gram_test.txt | python GetIndexandOtherWords.py --index-range 5-10     \\\n",
    "                                                      --top-n-words 10       \\\n",
    "                                                      --return-counts True   \\\n",
    "                                                       -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t[\"study\", 604]\r\n",
      "\"index\"\t[\"female\", 447]\r\n",
      "\"index\"\t[\"collection\", 239]\r\n",
      "\"index\"\t[\"fairy\", 123]\r\n",
      "\"index\"\t[\"tales\", 123]\r\n",
      "\"index\"\t[\"forms\", 116]\r\n",
      "\"words\"\t[\"child\", 1099]\r\n",
      "\"words\"\t[\"christmas\", 1099]\r\n",
      "\"words\"\t[\"wales\", 1099]\r\n",
      "\"words\"\t[\"case\", 604]\r\n",
      "\"words\"\t[\"study\", 604]\r\n",
      "\"words\"\t[\"female\", 447]\r\n",
      "\"words\"\t[\"collection\", 239]\r\n",
      "\"words\"\t[\"fairy\", 123]\r\n",
      "\"words\"\t[\"tales\", 123]\r\n",
      "\"words\"\t[\"forms\", 116]\r\n",
      "\"words\"\t[\"government\", 102]\r\n",
      "\"words\"\t[\"biography\", 92]\r\n",
      "\"words\"\t[\"general\", 92]\r\n",
      "\"words\"\t[\"george\", 92]\r\n",
      "\"words\"\t[\"circumstantial\", 62]\r\n",
      "\"words\"\t[\"city\", 62]\r\n",
      "\"words\"\t[\"narrative\", 62]\r\n",
      "\"words\"\t[\"sea\", 62]\r\n",
      "\"words\"\t[\"bill\", 59]\r\n",
      "\"words\"\t[\"establishing\", 59]\r\n",
      "\"words\"\t[\"religious\", 59]\r\n",
      "\"words\"\t[\"limited\", 55]\r\n"
     ]
    }
   ],
   "source": [
    "# Top words can be set as large as you want without error. Uses all possible words\n",
    "# if top-n-words > number of possible unique words in document.\n",
    "!cat 5gram_test.txt | python GetIndexandOtherWords.py --index-range 5-10     \\\n",
    "                                                      --top-n-words 10000    \\\n",
    "                                                      --return-counts True   \\\n",
    "                                                       -q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

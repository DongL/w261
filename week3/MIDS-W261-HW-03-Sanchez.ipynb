{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATSCIW261 ASSIGNMENT \n",
    "Version 2016-01-27 (FINAL)\n",
    "Week 3 ASSIGNMENTS\n",
    "\n",
    "Jason Sanchez - Group 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "\n",
    "* Merge sort.\n",
    "\n",
    "\n",
    "Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "\n",
    "* It is used after files are spilled to disk from the circular buffer before the map-side combiner is run as well as after the data is partitioned during the \"Hadoop shuffle\" and before it is fed into the reduce-side combiner.\n",
    "\n",
    "\n",
    "What is  a combiner function in the context of Hadoop? \n",
    "\n",
    "* Combiners improve the speed of MapReduce jobs. Map-side combiners can reduce the amount of data that needs to be transferred over the network by acting as a simplified reducer. Also, long running map jobs block the merge sort that is part of the \"Hadoop shuffle\" phase. A map-side combiner can run on all of the data that has been processed by the mappers before being blocked by the merge-sort. \n",
    "\n",
    "\n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "* Word count. Greatly reduce data needed to transfer of the network by combining key-value pairs. \n",
    "\n",
    "\n",
    "What is the Hadoop shuffle?\n",
    "\n",
    "* Partition --> Merge sort --> Pass to reduce-side combiner (or directly to reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumes you have downloaded the file and put it in the \"Temp_data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ComplaintDistribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ComplaintDistribution.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class ComplaintDistribution(MRJob):\n",
    "    def mapper(self, _, lines):\n",
    "        line = lines[:30]\n",
    "        if \"Debt collection\" in line:\n",
    "            self.increment_counter('Complaint', 'Debt collection', 1)\n",
    "        elif \"Mortgage\" in line:\n",
    "            self.increment_counter('Complaint', 'Mortgage', 1)\n",
    "        else:\n",
    "            self.increment_counter('Complaint', 'Other', 1)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    ComplaintDistribution.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/ComplaintDistribution.Jason.20160920.074657.421852\n",
      "Running step 1 of 1...\n",
      "Counters: 3\n",
      "\tComplaint\n",
      "\t\tDebt collection=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142789\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/ComplaintDistribution.Jason.20160920.074657.421852/output...\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/ComplaintDistribution.Jason.20160920.074657.421852...\n",
      "CPU times: user 28.7 ms, sys: 17.1 ms, total: 45.8 ms\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ComplaintDistribution.py Temp_data/Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "#### For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "#### Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SimpleCounters.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SimpleCounters.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SimpleCounters(MRJob):\n",
    "    def mapper_init(self):\n",
    "        self.increment_counter(\"Mappers\", \"Count\", 1)\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        self.increment_counter(\"Mappers\", \"Tasks\", 1)\n",
    "        for word in lines.split():\n",
    "            yield (word, 1)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.increment_counter(\"Reducers\", \"Count\", 1)\n",
    "    \n",
    "    def reducer(self, word, count):\n",
    "        self.increment_counter(\"Reducers\", \"Tasks\", 1)\n",
    "        yield (word, sum(count))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    SimpleCounters.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/SimpleCounters.Jason.20160920.082703.788299\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Counters: 2\n",
      "\tMappers\n",
      "\t\tCount=1\n",
      "\t\tTasks=1\n",
      "Counters: 4\n",
      "\tMappers\n",
      "\t\tCount=1\n",
      "\t\tTasks=1\n",
      "\tReducers\n",
      "\t\tCount=2\n",
      "\t\tTasks=4\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/SimpleCounters.Jason.20160920.082703.788299/output...\n",
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/SimpleCounters.Jason.20160920.082703.788299...\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | python SimpleCounters.py --jobconf mapred.map.tasks=2 --jobconf mapred.reduce.tasks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "#### Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting IssueCounter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile IssueCounter.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class IssueCounter(MRJob):\n",
    "\n",
    "    def mapper(self, _, lines):\n",
    "        self.increment_counter(\"Mappers\", \"Tasks\", 1)\n",
    "        terms = list(csv.reader([lines]))[0]\n",
    "        yield (terms[3], 1)\n",
    "    \n",
    "    def reducer(self, word, count):\n",
    "        self.increment_counter(\"Reducers\", \"Tasks\", 1)\n",
    "        self.increment_counter(\"Reducers\", \"Lines processed\", len(list(count)))\n",
    "        yield (word, sum(count))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    IssueCounter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/IssueCounter.Jason.20160921.070459.209407\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Counters: 1\n",
      "\tMappers\n",
      "\t\tTasks=312913\n",
      "Counters: 3\n",
      "\tMappers\n",
      "\t\tTasks=312913\n",
      "\tReducers\n",
      "\t\tLines processed=312913\n",
      "\t\tTasks=80\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/IssueCounter.Jason.20160921.070459.209407/output...\n",
      "\"\"\t0\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/IssueCounter.Jason.20160921.070459.209407...\n"
     ]
    }
   ],
   "source": [
    "!cat Temp_data/Consumer_Complaints.csv | python IssueCounter.py | head -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper tasks = 312913. The mapper was called this many times because that is how many lines there are in the file.\n",
    "\n",
    "Reducer tasks = 80. The reducer was called this many times because that is how many unique issues there are in the file.\n",
    "\n",
    "Reducer lines processed = 312913. The reducer was passed all of the data from the mappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  312913 Temp_data/Consumer_Complaints.csv\r\n"
     ]
    }
   ],
   "source": [
    "# We can easily confirm the first hypothesis\n",
    "!wc -l Temp_data/Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting IssueCounterCombiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile IssueCounterCombiner.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class IssueCounterCombiner(MRJob):\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        self.increment_counter(\"Mappers\", \"Tasks\", 1)\n",
    "        terms = list(csv.reader([lines]))[0]\n",
    "        yield (terms[3], 1)\n",
    "    \n",
    "    def combiner(self, word, count):\n",
    "        self.increment_counter(\"Combiners\", \"Tasks\", 1)\n",
    "        yield (word, sum(count))\n",
    "    \n",
    "    def reducer(self, word, count):\n",
    "        self.increment_counter(\"Reducers\", \"Tasks\", 1)\n",
    "        self.increment_counter(\"Reducers\", \"Lines processed\", len(list(count)))\n",
    "        yield (word, sum(count))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    IssueCounterCombiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python_mr_driver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile python_mr_driver.py\n",
    "\n",
    "from IssueCounterCombiner import IssueCounterCombiner\n",
    "\n",
    "mr_job = IssueCounterCombiner(args=['Temp_data/Consumer_Complaints.csv'])\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run() \n",
    "    print(runner.counters())\n",
    "#     for line in runner.stream_output(): \n",
    "#         print(mr_job.parse_output_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = !python python_mr_driver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[{'Combiners': {'Tasks': 146}, 'Reducers': {'Tasks': 80, 'Lines processed': 146}, 'Mappers': {'Tasks': 312913}}]\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the same amount of map and reduce tasks were called, because 146 combiner tasks were called, my hypothesis would be that the number of observations read by reducers was less. I went back and included a counter that kept track of the lines passed over the network. With the combiner, only 146 observations were passed over the network. This is equal to the number of times the combiner was called (which makes sense because combiners act as map-side reducers and each one would process on a different key of the data and output a single line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Top50.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Top50.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def order_key(order_in_reducer, key_name):\n",
    "    number_of_stars = order_in_reducer//10 + 1\n",
    "    number = str(order_in_reducer%10)\n",
    "    return \"%s %s\" % (\"*\"*number_of_stars+number, key_name)\n",
    "\n",
    "class Top50(MRJob):\n",
    "\n",
    "    MRJob.SORT_VALUES = True\n",
    "        \n",
    "    def mapper_get_issue(self, _, lines):\n",
    "        terms = list(csv.reader([lines]))[0]\n",
    "        issue = terms[3]\n",
    "        if issue == \"\":\n",
    "            issue = \"<blank>\"\n",
    "        yield (issue, 1)\n",
    "    \n",
    "    def combiner_count_issues(self, word, count):\n",
    "        yield (word, sum(count))\n",
    "        \n",
    "    def reducer_init_totals(self):\n",
    "        self.issue_counts = []\n",
    "    \n",
    "    def reducer_count_issues(self, word, count):\n",
    "        issue_count = sum(count)\n",
    "        self.issue_counts.append(int(issue_count))\n",
    "        yield (word, issue_count)\n",
    "        \n",
    "    def reducer_final_emit_counts(self):\n",
    "        yield (order_key(1, \"Total\"), sum(self.issue_counts))\n",
    "        yield (order_key(2, \"40th\"), sorted(self.issue_counts)[-40])\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.increment_counter(\"Reducers\", \"Count\", 1)\n",
    "        self.var = {}\n",
    "    \n",
    "    def reducer(self, word, count):\n",
    "        if word.startswith(\"*\"):\n",
    "            _, term = word.split()\n",
    "            self.var[term] = next(count)\n",
    "\n",
    "        else:\n",
    "            total = sum(count)\n",
    "            if total >= self.var[\"40th\"]:\n",
    "                yield (word, (total/self.var[\"Total\"], total))\n",
    "                \n",
    "    def mapper_sort(self, key, value):\n",
    "        value[0] = 1-float(value[0])\n",
    "        yield value, key\n",
    "        \n",
    "    def reducer_sort(self, key, value):\n",
    "        key[0] = round(1-float(key[0]),3)\n",
    "        yield key, next(value)\n",
    "\n",
    "    def steps(self):\n",
    "        mr_steps = [MRStep(mapper=self.mapper_get_issue,\n",
    "                           combiner=self.combiner_count_issues,\n",
    "                           reducer_init=self.reducer_init_totals,\n",
    "                           reducer=self.reducer_count_issues,\n",
    "                           reducer_final=self.reducer_final_emit_counts),\n",
    "                    MRStep(reducer_init=self.reducer_init,\n",
    "                           reducer=self.reducer),\n",
    "                    MRStep(mapper=self.mapper_sort,\n",
    "                           reducer=self.reducer_sort)\n",
    "                   ]\n",
    "        return mr_steps\n",
    "    \n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    Top50.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/Top50.Jason.20160926.010457.383781\n",
      "Running step 1 of 3...\n",
      "reading from STDIN\n",
      "Running step 2 of 3...\n",
      "Counters: 1\n",
      "\tReducers\n",
      "\t\tCount=1\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/Top50.Jason.20160926.010457.383781/output...\n",
      "[0.174, 522]\t\"Incorrect information on credit report\"\n",
      "[0.116, 349]\t\"Cont'd attempts collect debt not owed\"\n",
      "[0.098, 294]\t\"Loan modification,collection,foreclosure\"\n",
      "[0.079, 237]\t\"Loan servicing, payments, escrow account\"\n",
      "[0.049, 146]\t\"Communication tactics\"\n",
      "[0.045, 136]\t\"Disclosure verification of debt\"\n",
      "[0.041, 124]\t\"Account opening, closing, or management\"\n",
      "[0.036, 109]\t\"Credit reporting company's investigation\"\n",
      "[0.028, 83]\t\"Deposits and withdrawals\"\n",
      "[0.021, 62]\t\"Managing the loan or lease\"\n",
      "[0.02, 61]\t\"False statements or representation\"\n",
      "[0.017, 50]\t\"Problems caused by my funds being low\"\n",
      "[0.016, 48]\t\"Improper contact or sharing of info\"\n",
      "[0.015, 44]\t\"Application, originator, mortgage broker\"\n",
      "[0.012, 35]\t\"Problems when you are unable to pay\"\n",
      "[0.011, 34]\t\"Other\"\n",
      "[0.011, 33]\t\"Billing disputes\"\n",
      "[0.009, 26]\t\"Using a debit or ATM card\"\n",
      "[0.007, 22]\t\"Closing/Cancelling account\"\n",
      "[0.007, 21]\t\"Improper use of my credit report\"\n",
      "[0.007, 20]\t\"APR or interest rate\"\n",
      "[0.006, 19]\t\"Credit decision / Underwriting\"\n",
      "[0.006, 17]\t\"Taking out the loan or lease\"\n",
      "[0.005, 16]\t\"Making/receiving payments, sending money\"\n",
      "[0.005, 15]\t\"Late fee\"\n",
      "[0.004, 13]\t\"Charged fees or interest I didn't expect\"\n",
      "[0.004, 12]\t\"Credit determination\"\n",
      "[0.004, 11]\t\"Shopping for a loan or lease\"\n",
      "[0.003, 10]\t\"Advertising and marketing\"\n",
      "[0.003, 9]\t\"Billing statement\"\n",
      "[0.003, 8]\t\"Fraud or scam\"\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/Top50.Jason.20160926.010457.383781...\n"
     ]
    }
   ],
   "source": [
    "!head -n 3001 Temp_data/Consumer_Complaints.csv | python Top50.py --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please **use a combiner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \r\n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \r\n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \r\n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO18919 DAI50921 SNA80192 GRO75578 \r\n",
      "ELE17451 ELE59935 FRO18919 ELE23393 SNA80192 SNA85662 SNA91554 DAI22177 \r\n",
      "ELE17451 SNA69641 FRO18919 SNA90258 ELE28573 ELE11375 DAI14125 FRO78087 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA80192 SNA85662 SNA90258 DAI46755 FRO81176 ELE66810 DAI49199 DAI91535 GRO94758 ELE94711 DAI22177 \r\n",
      "ELE17451 SNA69641 DAI91535 GRO94758 GRO99222 FRO76833 FRO81176 SNA80192 DAI54690 ELE37798 GRO56989 \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 Temp_data/ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ProductPurchaseStats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProductPurchaseStats.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import heapq\n",
    "\n",
    "\n",
    "class TopList(list):\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"\n",
    "        Just like a list, except the append method adds the new value to the \n",
    "        list only if it is larger than the smallest value (or if the size of \n",
    "        the list is less than max_size). If each element of the list is an int\n",
    "        or float, uses that value for comparison. If the first element is a \n",
    "        list or tuple, uses the first element of the list or tuple for the \n",
    "        comparison.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def _get_key(self, x):\n",
    "        return x[0] if isinstance(x, (list, tuple)) else x\n",
    "        \n",
    "    def append(self, val):\n",
    "        key=lambda x: x[0] if isinstance(x, (list, tuple)) else x\n",
    "        if len(self) < self.max_size:\n",
    "            heapq.heappush(self, val)\n",
    "        elif self._get_key(self[0]) < self._get_key(val):\n",
    "            heapq.heapreplace(self, val)\n",
    "            \n",
    "    def final_sort(self):\n",
    "        return sorted(self, key=self._get_key, reverse=True)\n",
    "\n",
    "\n",
    "class ProductPurchaseStats(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.largest_basket = 0\n",
    "        self.total_items = 0\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        products = lines.split()\n",
    "        n_products = len(products)\n",
    "        self.total_items += n_products\n",
    "        if n_products > self.largest_basket:\n",
    "            self.largest_basket = n_products\n",
    "        for prod in products:\n",
    "            yield (prod, 1)\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        self.increment_counter(\"product stats\", \"largest basket\", self.largest_basket)\n",
    "        yield (\"*** Total\", self.total_items)\n",
    "        \n",
    "    def combiner(self, keys, values):\n",
    "        yield keys, sum(values)\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.top50 = TopList(50)\n",
    "        self.total = 0\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        value_count = sum(values)\n",
    "        \n",
    "        if key == \"*** Total\":\n",
    "            self.total = value_count\n",
    "        else:\n",
    "            self.increment_counter(\"product stats\", \"unique products\")\n",
    "            self.top50.append([value_count, value_count/self.total, key])\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for counts, relative_rate, key in self.top50.final_sort():\n",
    "            yield key, (counts, round(relative_rate,3))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ProductPurchaseStats.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/ProductPurchaseStats.Jason.20160926.034901.357619\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Counters: 1\n",
      "\tproduct stats\n",
      "\t\tlargest basket=74\n",
      "Counters: 2\n",
      "\tproduct stats\n",
      "\t\tlargest basket=74\n",
      "\t\tunique products=12592\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/ProductPurchaseStats.Jason.20160926.034901.357619/output...\n",
      "\"DAI62779\"\t[6667, 0.018]\n",
      "\"FRO40251\"\t[3881, 0.01]\n",
      "\"ELE17451\"\t[3875, 0.01]\n",
      "\"GRO73461\"\t[3602, 0.009]\n",
      "\"SNA80324\"\t[3044, 0.008]\n",
      "\"ELE32164\"\t[2851, 0.007]\n",
      "\"DAI75645\"\t[2736, 0.007]\n",
      "\"SNA45677\"\t[2455, 0.006]\n",
      "\"FRO31317\"\t[2330, 0.006]\n",
      "\"DAI85309\"\t[2293, 0.006]\n",
      "\"ELE26917\"\t[2292, 0.006]\n",
      "\"FRO80039\"\t[2233, 0.006]\n",
      "\"GRO21487\"\t[2115, 0.006]\n",
      "\"SNA99873\"\t[2083, 0.005]\n",
      "\"GRO59710\"\t[2004, 0.005]\n",
      "\"GRO71621\"\t[1920, 0.005]\n",
      "\"FRO85978\"\t[1918, 0.005]\n",
      "\"GRO30386\"\t[1840, 0.005]\n",
      "\"ELE74009\"\t[1816, 0.005]\n",
      "\"GRO56726\"\t[1784, 0.005]\n",
      "\"DAI63921\"\t[1773, 0.005]\n",
      "\"GRO46854\"\t[1756, 0.005]\n",
      "\"ELE66600\"\t[1713, 0.004]\n",
      "\"DAI83733\"\t[1712, 0.004]\n",
      "\"FRO32293\"\t[1702, 0.004]\n",
      "\"ELE66810\"\t[1697, 0.004]\n",
      "\"SNA55762\"\t[1646, 0.004]\n",
      "\"DAI22177\"\t[1627, 0.004]\n",
      "\"FRO78087\"\t[1531, 0.004]\n",
      "\"ELE99737\"\t[1516, 0.004]\n",
      "\"ELE34057\"\t[1489, 0.004]\n",
      "\"GRO94758\"\t[1489, 0.004]\n",
      "\"FRO35904\"\t[1436, 0.004]\n",
      "\"FRO53271\"\t[1420, 0.004]\n",
      "\"SNA93860\"\t[1407, 0.004]\n",
      "\"SNA90094\"\t[1390, 0.004]\n",
      "\"GRO38814\"\t[1352, 0.004]\n",
      "\"ELE56788\"\t[1345, 0.004]\n",
      "\"GRO61133\"\t[1321, 0.003]\n",
      "\"ELE74482\"\t[1316, 0.003]\n",
      "\"DAI88807\"\t[1316, 0.003]\n",
      "\"ELE59935\"\t[1311, 0.003]\n",
      "\"SNA96271\"\t[1295, 0.003]\n",
      "\"DAI43223\"\t[1290, 0.003]\n",
      "\"ELE91337\"\t[1289, 0.003]\n",
      "\"GRO15017\"\t[1275, 0.003]\n",
      "\"DAI31081\"\t[1261, 0.003]\n",
      "\"GRO81087\"\t[1220, 0.003]\n",
      "\"DAI22896\"\t[1219, 0.003]\n",
      "\"GRO85051\"\t[1214, 0.003]\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/ProductPurchaseStats.Jason.20160926.034901.357619...\n"
     ]
    }
   ],
   "source": [
    "!cat Temp_data/ProductPurchaseData.txt | python ProductPurchaseStats.py  --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PairsRecommender.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PairsRecommender.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import heapq\n",
    "import sys\n",
    "\n",
    "def all_itemsets_of_size_two(array, key=None, return_type=\"string\", concat_val=\" \"):\n",
    "    \"\"\"\n",
    "    Generator that yields all valid itemsets of size two\n",
    "    where each combo is returned in an order sorted by key.\n",
    "    \n",
    "    key = None defaults to standard sorting.\n",
    "    \n",
    "    return_type: can be \"string\" or \"tuple\". If \"string\", \n",
    "    concatenates values with concat_val and returns string.\n",
    "    If tuple, returns a tuple with two elements.\n",
    "    \"\"\"\n",
    "    array = sorted(array, key=key)\n",
    "    for index, item in enumerate(array):\n",
    "        for other_item in array[index:]:\n",
    "            if item != other_item:\n",
    "                if return_type == \"string\":\n",
    "                    yield \"%s%s%s\" % (str(item), concat_val, str(other_item))\n",
    "                else:\n",
    "                    yield (item, other_item) \n",
    "\n",
    "class TopList(list):\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"\n",
    "        Just like a list, except the append method adds the new value to the \n",
    "        list only if it is larger than the smallest value (or if the size of \n",
    "        the list is less than max_size). If each element of the list is an int\n",
    "        or float, uses that value for comparison. If the first element is a \n",
    "        list or tuple, uses the first element of the list or tuple for the \n",
    "        comparison.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def _get_key(self, x):\n",
    "        return x[0] if isinstance(x, (list, tuple)) else x\n",
    "        \n",
    "    def append(self, val):\n",
    "        key=lambda x: x[0] if isinstance(x, (list, tuple)) else x\n",
    "        if len(self) < self.max_size:\n",
    "            heapq.heappush(self, val)\n",
    "        elif self._get_key(self[0]) < self._get_key(val):\n",
    "            heapq.heapreplace(self, val)\n",
    "            \n",
    "    def final_sort(self):\n",
    "        return sorted(self, key=self._get_key, reverse=True)\n",
    "    \n",
    "                    \n",
    "class PairsRecommender(MRJob):\n",
    "    def mapper_init(self):\n",
    "        self.total_baskets = 0\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        self.total_baskets += 1\n",
    "        products = lines.split()\n",
    "        self.increment_counter(\"job stats\", \"number of items\", len(products))\n",
    "        for itemset in all_itemsets_of_size_two(products):\n",
    "            self.increment_counter(\"job stats\", \"number of item combos\")\n",
    "            yield (itemset, 1)\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        self.increment_counter(\"job stats\", \"number of baskets\", self.total_baskets)\n",
    "        yield (\"*** Total\", self.total_baskets)\n",
    "        \n",
    "    def combiner(self, key, values):\n",
    "        self.increment_counter(\"job stats\", \"number of keys fed to combiner\")\n",
    "        yield key, sum(values)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.top_values = TopList(50)\n",
    "        self.total_baskets = 0\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        values_sum = sum(values)\n",
    "        if key == \"*** Total\":\n",
    "            self.total_baskets = values_sum\n",
    "        elif values_sum >= 100:\n",
    "            self.increment_counter(\"job stats\", \"number of unique itemsets >= 100\")\n",
    "            basket_percent = values_sum/self.total_baskets\n",
    "            self.top_values.append([values_sum, round(basket_percent,3), key])\n",
    "        else:\n",
    "            self.increment_counter(\"job stats\", \"number of unique itemsets < 100\")\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        for values_sum, basket_percent, key in self.top_values.final_sort():\n",
    "            yield key, (values_sum, basket_percent)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    PairsRecommender.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/PairsRecommender.Jason.20160926.045124.502699\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Counters: 4\n",
      "\tjob stats\n",
      "\t\tnumber of baskets=31101\n",
      "\t\tnumber of item combos=2534054\n",
      "\t\tnumber of items=380824\n",
      "\t\tnumber of keys fed to combiner=1026709\n",
      "Counters: 6\n",
      "\tjob stats\n",
      "\t\tnumber of baskets=31101\n",
      "\t\tnumber of item combos=2534054\n",
      "\t\tnumber of items=380824\n",
      "\t\tnumber of itemsets < 100=875761\n",
      "\t\tnumber of itemsets >= 100=1334\n",
      "\t\tnumber of keys fed to combiner=1026709\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/PairsRecommender.Jason.20160926.045124.502699/output...\n",
      "\"DAI62779 ELE17451\"\t[1592, 0.051]\n",
      "\"FRO40251 SNA80324\"\t[1412, 0.045]\n",
      "\"DAI75645 FRO40251\"\t[1254, 0.04]\n",
      "\"FRO40251 GRO85051\"\t[1213, 0.039]\n",
      "\"DAI62779 GRO73461\"\t[1139, 0.037]\n",
      "\"DAI75645 SNA80324\"\t[1130, 0.036]\n",
      "\"DAI62779 FRO40251\"\t[1070, 0.034]\n",
      "\"DAI62779 SNA80324\"\t[923, 0.03]\n",
      "\"DAI62779 DAI85309\"\t[918, 0.03]\n",
      "\"ELE32164 GRO59710\"\t[911, 0.029]\n",
      "\"DAI62779 DAI75645\"\t[882, 0.028]\n",
      "\"FRO40251 GRO73461\"\t[882, 0.028]\n",
      "\"DAI62779 ELE92920\"\t[877, 0.028]\n",
      "\"FRO40251 FRO92469\"\t[835, 0.027]\n",
      "\"DAI62779 ELE32164\"\t[832, 0.027]\n",
      "\"DAI75645 GRO73461\"\t[712, 0.023]\n",
      "\"DAI43223 ELE32164\"\t[711, 0.023]\n",
      "\"DAI62779 GRO30386\"\t[709, 0.023]\n",
      "\"ELE17451 FRO40251\"\t[697, 0.022]\n",
      "\"DAI85309 ELE99737\"\t[659, 0.021]\n",
      "\"DAI62779 ELE26917\"\t[650, 0.021]\n",
      "\"GRO21487 GRO73461\"\t[631, 0.02]\n",
      "\"DAI62779 SNA45677\"\t[604, 0.019]\n",
      "\"ELE17451 SNA80324\"\t[597, 0.019]\n",
      "\"DAI62779 GRO71621\"\t[595, 0.019]\n",
      "\"DAI62779 SNA55762\"\t[593, 0.019]\n",
      "\"DAI62779 DAI83733\"\t[586, 0.019]\n",
      "\"ELE17451 GRO73461\"\t[580, 0.019]\n",
      "\"GRO73461 SNA80324\"\t[562, 0.018]\n",
      "\"DAI62779 GRO59710\"\t[561, 0.018]\n",
      "\"DAI62779 FRO80039\"\t[550, 0.018]\n",
      "\"DAI75645 ELE17451\"\t[547, 0.018]\n",
      "\"DAI62779 SNA93860\"\t[537, 0.017]\n",
      "\"DAI55148 DAI62779\"\t[526, 0.017]\n",
      "\"DAI43223 GRO59710\"\t[512, 0.016]\n",
      "\"ELE17451 ELE32164\"\t[511, 0.016]\n",
      "\"DAI62779 SNA18336\"\t[506, 0.016]\n",
      "\"ELE32164 GRO73461\"\t[486, 0.016]\n",
      "\"DAI62779 FRO78087\"\t[482, 0.015]\n",
      "\"DAI85309 ELE17451\"\t[482, 0.015]\n",
      "\"DAI62779 GRO94758\"\t[479, 0.015]\n",
      "\"DAI62779 GRO21487\"\t[471, 0.015]\n",
      "\"GRO85051 SNA80324\"\t[471, 0.015]\n",
      "\"ELE17451 GRO30386\"\t[468, 0.015]\n",
      "\"FRO85978 SNA95666\"\t[463, 0.015]\n",
      "\"DAI62779 FRO19221\"\t[462, 0.015]\n",
      "\"DAI62779 GRO46854\"\t[461, 0.015]\n",
      "\"DAI43223 DAI62779\"\t[459, 0.015]\n",
      "\"ELE92920 SNA18336\"\t[455, 0.015]\n",
      "\"DAI88079 FRO40251\"\t[446, 0.014]\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/PairsRecommender.Jason.20160926.045124.502699...\n",
      "CPU times: user 770 ms, sys: 206 ms, total: 975 ms\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/ProductPurchaseData.txt | python PairsRecommender.py  --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware:\r\n",
      "\r\n",
      "    Hardware Overview:\r\n",
      "\r\n",
      "      Model Name: MacBook Pro\r\n",
      "      Model Identifier: MacBookPro12,1\r\n",
      "      Processor Name: Intel Core i7\r\n",
      "      Processor Speed: 3.1 GHz\r\n",
      "      Number of Processors: 1\r\n",
      "      Total Number of Cores: 2\r\n",
      "      L2 Cache (per Core): 256 KB\r\n",
      "      L3 Cache: 4 MB\r\n",
      "      Memory: 16 GB\r\n",
      "      Boot ROM Version: MBP121.0167.B17\r\n",
      "      SMC Version (system): 2.28f7\r\n",
      "      Serial Number (system): C02RT071FVH9\r\n",
      "      Hardware UUID: D12CBB32-4EFD-5F0A-83B5-3E6FE291C8E1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!system_profiler SPHardwareDataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OPTIONAL: all HW below this are optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting StripesRecommender.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile StripesRecommender.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "import sys\n",
    "import heapq\n",
    "\n",
    "def all_itemsets_of_size_two_stripes(array, key=None):\n",
    "    \"\"\"\n",
    "    Generator that yields all valid itemsets of size two\n",
    "    where each combo is as a stripe.\n",
    "    \n",
    "    key = None defaults to standard sorting.\n",
    "    \"\"\"\n",
    "    array = sorted(array, key=key)\n",
    "    for index, item in enumerate(array[:-1]):\n",
    "        yield (item, {key:1 for key in array[index+1:]})\n",
    "\n",
    "class TopList(list):\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"\n",
    "        Just like a list, except the append method adds the new value to the \n",
    "        list only if it is larger than the smallest value (or if the size of \n",
    "        the list is less than max_size). If each element of the list is an int\n",
    "        or float, uses that value for comparison. If the first element is a \n",
    "        list or tuple, uses the first element of the list or tuple for the \n",
    "        comparison.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def _get_key(self, x):\n",
    "        return x[0] if isinstance(x, (list, tuple)) else x\n",
    "        \n",
    "    def append(self, val):\n",
    "        key=lambda x: x[0] if isinstance(x, (list, tuple)) else x\n",
    "        if len(self) < self.max_size:\n",
    "            heapq.heappush(self, val)\n",
    "        elif self._get_key(self[0]) < self._get_key(val):\n",
    "            heapq.heapreplace(self, val)\n",
    "            \n",
    "    def final_sort(self):\n",
    "        return sorted(self, key=self._get_key, reverse=True)\n",
    "    \n",
    "        \n",
    "class StripesRecommender(MRJob):\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.basket_count = 0\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        self.basket_count += 1\n",
    "        products = lines.split()\n",
    "        for item, value in all_itemsets_of_size_two_stripes(products):\n",
    "            yield item, value\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        yield (\"*** Total\", {\"total\": self.basket_count})\n",
    "        \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.top = TopList(50)\n",
    "    \n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "\n",
    "        if keys == \"*** Total\":            \n",
    "            self.total = values_sum[\"total\"]\n",
    "        else:\n",
    "            for k, v in values_sum.items():\n",
    "                if v >= 100:\n",
    "                    self.top.append([v, round(v/self.total,3), keys+\" \"+k])\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for count, perc, key in self.top.final_sort():\n",
    "            yield key, (count, perc)\n",
    "                    \n",
    "if __name__ == \"__main__\":\n",
    "    StripesRecommender.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/StripesRecommender.Jason.20160926.054535.885306\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/StripesRecommender.Jason.20160926.054535.885306/output...\n",
      "\"DAI62779 ELE17451\"\t[1592, 0.051]\n",
      "\"FRO40251 SNA80324\"\t[1412, 0.045]\n",
      "\"DAI75645 FRO40251\"\t[1254, 0.04]\n",
      "\"FRO40251 GRO85051\"\t[1213, 0.039]\n",
      "\"DAI62779 GRO73461\"\t[1139, 0.037]\n",
      "\"DAI75645 SNA80324\"\t[1130, 0.036]\n",
      "\"DAI62779 FRO40251\"\t[1070, 0.034]\n",
      "\"DAI62779 SNA80324\"\t[923, 0.03]\n",
      "\"DAI62779 DAI85309\"\t[918, 0.03]\n",
      "\"ELE32164 GRO59710\"\t[911, 0.029]\n",
      "\"DAI62779 DAI75645\"\t[882, 0.028]\n",
      "\"FRO40251 GRO73461\"\t[882, 0.028]\n",
      "\"DAI62779 ELE92920\"\t[877, 0.028]\n",
      "\"FRO40251 FRO92469\"\t[835, 0.027]\n",
      "\"DAI62779 ELE32164\"\t[832, 0.027]\n",
      "\"DAI75645 GRO73461\"\t[712, 0.023]\n",
      "\"DAI43223 ELE32164\"\t[711, 0.023]\n",
      "\"DAI62779 GRO30386\"\t[709, 0.023]\n",
      "\"ELE17451 FRO40251\"\t[697, 0.022]\n",
      "\"DAI85309 ELE99737\"\t[659, 0.021]\n",
      "\"DAI62779 ELE26917\"\t[650, 0.021]\n",
      "\"GRO21487 GRO73461\"\t[631, 0.02]\n",
      "\"DAI62779 SNA45677\"\t[604, 0.019]\n",
      "\"ELE17451 SNA80324\"\t[597, 0.019]\n",
      "\"DAI62779 GRO71621\"\t[595, 0.019]\n",
      "\"DAI62779 SNA55762\"\t[593, 0.019]\n",
      "\"DAI62779 DAI83733\"\t[586, 0.019]\n",
      "\"ELE17451 GRO73461\"\t[580, 0.019]\n",
      "\"GRO73461 SNA80324\"\t[562, 0.018]\n",
      "\"DAI62779 GRO59710\"\t[561, 0.018]\n",
      "\"DAI62779 FRO80039\"\t[550, 0.018]\n",
      "\"DAI75645 ELE17451\"\t[547, 0.018]\n",
      "\"DAI62779 SNA93860\"\t[537, 0.017]\n",
      "\"DAI55148 DAI62779\"\t[526, 0.017]\n",
      "\"DAI43223 GRO59710\"\t[512, 0.016]\n",
      "\"ELE17451 ELE32164\"\t[511, 0.016]\n",
      "\"DAI62779 SNA18336\"\t[506, 0.016]\n",
      "\"ELE32164 GRO73461\"\t[486, 0.016]\n",
      "\"DAI85309 ELE17451\"\t[482, 0.015]\n",
      "\"DAI62779 FRO78087\"\t[482, 0.015]\n",
      "\"DAI62779 GRO94758\"\t[479, 0.015]\n",
      "\"DAI62779 GRO21487\"\t[471, 0.015]\n",
      "\"GRO85051 SNA80324\"\t[471, 0.015]\n",
      "\"ELE17451 GRO30386\"\t[468, 0.015]\n",
      "\"FRO85978 SNA95666\"\t[463, 0.015]\n",
      "\"DAI62779 FRO19221\"\t[462, 0.015]\n",
      "\"DAI62779 GRO46854\"\t[461, 0.015]\n",
      "\"DAI43223 DAI62779\"\t[459, 0.015]\n",
      "\"ELE92920 SNA18336\"\t[455, 0.015]\n",
      "\"DAI88079 FRO40251\"\t[446, 0.014]\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/StripesRecommender.Jason.20160926.054535.885306...\n",
      "CPU times: user 209 ms, sys: 59.2 ms, total: 268 ms\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/ProductPurchaseData.txt | python StripesRecommender.py  --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairs operation took 1 minute 30 seconds. The stripes operation took 24 seconds, which is about a quarter of the time for pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)\n",
    "\n",
    "Dataset description\n",
    "For this assignment you will explore a set of 100,000 Wikipedia documents:\n",
    "\n",
    "https://www.dropbox.com/s/n5lfbnztclo93ej/wikitext_100k.txt?dl=0\n",
    "s3://cs9223/wikitext_100k.txt, or\n",
    "https://s3.amazonaws.com/cs9223/wikitext_100k.txt\n",
    "Each line in this file consists of the plain text extracted from a Wikipedia document.\n",
    "\n",
    "Task\n",
    "Compute the relative frequencies of each word that occurs in the documents in wikitext_100k.txt and output the top 100 word pairs sorted by decreasing order of relative frequency.\n",
    "\n",
    "Recall that the relative frequency (RF) of word B given word A is defined as follows:\n",
    "\n",
    "   f(B|A) = Count(A, B) / Count (A)   =  Count(A, B) / sum_B'(Count (A, B')\n",
    "\n",
    "where count(A,B) is the number of times A and B co-occur within a window of two words (co-occurrence window size of two) in a document and count(A) the number of times A occurs with anything else. Intuitively, given a document collection, the relative frequency captures the proportion of time the word B appears in the same document as A. (See Section 3.3, in Data-Intensive Text Processing with MapReduce).\n",
    "\n",
    "In the async lecture you learned different approaches to do this, and in this assignment, you will implement them:\n",
    "\n",
    "a.\tWrite a mapreduce program which uses the Stripes approach and writes its output in a file named rfstripes.txt \n",
    "\n",
    "b.\tWrite a mapreduce program which uses the Pairs approach and writes its output in a file named rfpairs.txt\n",
    "\n",
    "c.\tCompare the performance of the two approaches and output the relative performance to a file named rfcomp.txt. Compute the relative performance as follows: (running time for Pairs/ running time for Stripes). Also include an analysis comparing the communication costs for the two approaches. Instrument your mapper and reduces for counters where necessary to aid with your analysis.\n",
    "\n",
    "NOTE: please limit your analysis to the top 100 word pairs sorted by decreasing order of relative frequency for each word (tokens with all alphabetical letters).\n",
    "\n",
    "Please include markdown cell named rf.txt that describes the following:\n",
    "\n",
    "the input/output format in each Hadoop task, i.e., the keys for the mappers and reducers\n",
    "the Hadoop cluster settings you used, i.e., number of mappers and reducers\n",
    "the running time for each approach: pairs and stripes\n",
    "\n",
    "You can write your program using Python or MrJob (with Hadoop streaming) and you should run it on AWS. It is a good idea to develop and test your program on a local machine  before deploying on AWS. Remember your notebook, needs to have all the commands you used to run each Mapreduce job (i.e., pairs and stripes) -- include the Hadoop streaming commands you used to run your jobs.\n",
    "\n",
    "In addition the All the following files should be compressed in one ZIP file and submitted. The ZIP file should contain:\n",
    "\n",
    "\n",
    "A.\tThe result files: rfstripes.txt, rfpairs.txt, rfcomp.txt\n",
    "\n",
    "Prior to working with Hadoop, the corpus should first be preprocessed as follows:\n",
    "perform tokenization (whitespace and all non-alphabetic characters) and stopword removal  using standard tools from the Lucene search engine. All tokens should  then be replaced\n",
    "with unique integers for a more efficient encoding. \n",
    "\n",
    "\n",
    "== Preliminary information for the remaing HW problems===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.7 Apriori Algorithm\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above): \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.8. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## HW3.8\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "END OF HOMEWORK\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][:-1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

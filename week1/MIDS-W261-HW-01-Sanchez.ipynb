{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW1\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Jason Sanchez*   (26989981)\n",
    "__Class:__ MIDS w261 (Section *Fall 2016 Group 2*)     \n",
    "__Email:__  *jason.sanchez*@iSchool.Berkeley.edu     \n",
    "__Week:__   1\n",
    "\n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time). I.e., Tuesday, Sept 6, 2016 in the case of this homework. \n",
    "\n",
    "__Submit Date:__ Sept 6, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "1.  [HW Introduction](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW  Problems](#3)   \n",
    "    1.0.  [HW1.0](#1.0)   \n",
    "    1.0.  [HW1.1](#1.1)   \n",
    "    1.2.  [HW1.2](#1.2)   \n",
    "    1.3.  [HW1.3](#1.3)    \n",
    "    1.4.  [HW1.4](#1.4)    \n",
    "    1.5.  [HW1.5](#1.5)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"1\">\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #1\n",
    "\n",
    "Version 2016-09-2 \n",
    "\n",
    " === INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "HW1 can be completed locally on your computer\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW1.0  <a name=\"1.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "### HW1.0.1.  Self-Introduction\n",
    "W1.0.0 Prepare your bio and include it in this HW submission. Please limit to 100 words. Count the words in your bio and print the length of your bio (in terms of words) in a separate cell.\n",
    "\n",
    "Fill in the following information [Optional]\n",
    "* Your Location \n",
    "* When did you start MIDS  and what is your target finish date\n",
    "* What you want to get out of w261?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%%writefile bio\n",
    "\n",
    "Hi! My name is Jason Sanchez. I live in San Francisco. I started MIDS in Fall 2015 and expect to finish in Summer 2017. Why so long? I am a cofounder of a startup that is using data science to modernize the insurance industry. \n",
    "\n",
    "After completing the first week of class, I am extremely hopeful that this class will define my Berkeley experience. This class seems difficult, but extremely worth the effort and I would like to feel comfortable tackling big data problems in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      86\r\n"
     ]
    }
   ],
   "source": [
    "!wc -w < bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.2. Big data\n",
    "Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Big data problems are ones that cannot be solved in a reasonable amount of time using one computer. \n",
    "\n",
    "Processing trip data from telematics devices (i.e. systems that monitor driving behavior) in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.3.  Bias Variance\n",
    "What is  bias-variance decomposition in the context machine learning? How is it used in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*bias*: The difference in predictions between the trained model and the true function that generated the data (not including the irreducible noise).\n",
    "\n",
    "*variance*: The variablility in the predictions of a model given different training data. \n",
    "\n",
    "Simple models (i.e. models that produce simple decision boundaries) have high bias and adding a lot more data won't dramatically increase the accuracy of the model. Such models are said to underfit the data because they do not have the ability to use the data available to better infer the true decision boundary. \n",
    "\n",
    "Complex models that produce highly complex decision boundaries tend to have high variance - especially in smaller datasets. This variance is reducible by adding more data. In fact, adding an infinite amount of data would drive the variance of any model down to zero (leaving only the bias).\n",
    "\n",
    "The problem is we don't have an infinite amount of data and we must make a tradeoff between bias and variance. By plotting the cross validation error rates attained from moving from a simple model to a complex model (in terms of the hyperparameters of the model), we can see where the inflection point between bias and variance occurs. \n",
    "\n",
    "When the model is simple, it misses a lot of the signal and thus the error is high. Increasing the complexity or flexibility of the model tends to increase the accuracy because more of the signal is being learned. Increasing the complexity of the model further can then lead to performance degradation because the model starts to learn patterns that only exist in the training dataset.\n",
    "\n",
    "Therefore, in machine learning we must balance the bias and variance of a model given the size of the dataset available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW1.1 WordCount using a single thread  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Write a program called alice_words.py that creates a text file named __alice_words.txt__ containing an alphabetical listing of all the words, and the number of times each occurs, in the text version of Alice’s Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org/cache/epub/11/pg11.txt) The first 10 lines of your output file should look something like this (the counts are not totally precise):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word              Count\n",
    "=======================\n",
    "a                 631\n",
    "a-piece           1\n",
    "abide             1\n",
    "able              1\n",
    "about             94\n",
    "above             3\n",
    "absence           1\n",
    "absurd            2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/BlueOwl1/Desktop/w261\r\n"
     ]
    }
   ],
   "source": [
    "# check where is the current directory and change if necessary using something like: %cd W261MasterDir\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Introduction to Information Retrieval.pdf\r\n",
      "CountOfMonteCristo.txt\r\n",
      "DivideAndConquer.ipynb\r\n",
      "DivideAndConquer.pdf\r\n",
      "LICENSE.txt\r\n",
      "MIDS-W261-HW-01-Sanchez.ipynb\r\n",
      "bio\r\n",
      "\u001b[31mmapper.py\u001b[m\u001b[m*\r\n",
      "\u001b[31mpGrepCount.sh\u001b[m\u001b[m*\r\n",
      "\u001b[31mreducer.py\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  163k  100  163k    0     0   175k      0 --:--:-- --:--:-- --:--:--  175k\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://www.gutenberg.org/cache/epub/11/pg11.txt' -o alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r",
      "\r\n",
      "with this eBook or online at www.gutenberg.org\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Title: Alice's Adventures in Wonderland\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#display the first few lines\n",
    "!head alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beck',\n",
       " 'global',\n",
       " 'risk',\n",
       " 'management',\n",
       " 'operations',\n",
       " 'congratulations',\n",
       " 'sally',\n",
       " 'kk',\n",
       " 'forwarded',\n",
       " 'by']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of a regular expression to detect words in a string. \n",
    "import re\n",
    "line = \"\"\" 0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy\"\"\"\n",
    "re.findall(r'[a-z]+', line.lower())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries are a good way to keep track of word counts\n",
    "\n",
    "wordCounts={} \n",
    "\n",
    "### defaultdict are slightly more effectice way of doing word counting\n",
    " One way to do word counting but not best. A defaultdict is like a regular dictionary, except that when you try to look up a key it doesn’t contain, it first adds a value for it using a zero-argument function you provided\n",
    "when you created it. In order to use defaultdicts, you have to import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 7\n",
      "accounting 1\n",
      "activities 3\n",
      "additional 1\n",
      "administration 1\n",
      "all 3\n",
      "allocation 1\n",
      "also 3\n",
      "america 2\n",
      "among 1\n"
     ]
    }
   ],
   "source": [
    "# Here is an example of wordcounting with a defaultdict (dictionary structure with a nice \n",
    "# default behaviours when a key does not exist in the dictionary\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "line = \"\"\" 0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy kokas/corp/enron on 01/17/2000  08:08 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\"\"\n",
    "wordCounts=defaultdict(int)\n",
    "for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    #if word in [\"a\"]:\n",
    "        #print word,\"\\n\"\n",
    "    wordCounts[word] += 1\n",
    "for key in sorted(wordCounts)[0:10]:\n",
    "    print (key, wordCounts[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1.1 How many times does the word alice occur in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", \"r\") as alice:\n",
    "    print(len(re.findall(r'alice+', alice.read().lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results doesn't make sense in light of the bottom result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", \"r\") as alice:\n",
    "    print(len(re.findall(r'[Aa]lice+', alice.read())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", \"r\") as alice:\n",
    "    print(len(re.findall(r'alice+', alice.read())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW1.2 Command Line Map Reduce Framework  <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Read through the provided mapreduce shell script (pWordCount.sh) provided below\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   Run the shell without any arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pWordCount.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pWordCount.sh\n",
    "#!/bin/bash\n",
    "## pWordCount.sh\n",
    "## Author: James G. Shanahan\n",
    "## Usage: pWordCount.sh m wordlist testFile.txt\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\" if set to \"*\", then all words are used\n",
    "##       inputFile = a text input file\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "\n",
    "usage()\n",
    "{\n",
    "    echo ERROR: No arguments supplied\n",
    "    echo\n",
    "    echo To run use\n",
    "    echo \"     pWordCount.sh m wordlist inputFile\"\n",
    "    echo Input:\n",
    "    echo \"      number of processes/maps, EG, 4\"\n",
    "    echo \"      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\" \n",
    "    echo \"      inputFile = a text input file\"\n",
    "}\n",
    "\n",
    "if [ $# -eq 0 ] # I removed a hash after the $ sign\n",
    "  then\n",
    "    usage  \n",
    "    exit 1\n",
    "fi\n",
    "    \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a text file \n",
    "data=$3\n",
    "    \n",
    "## Clean up remaining chunks from a previous run\n",
    "rm -f $data.chunk.*\n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py  \"$wordlist\" <$datachunk > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "###----------------------------------------------------------------------------------------\n",
    "# Sorting magic\n",
    "    \n",
    "hash_to_bucket ()\n",
    "{\n",
    "# Takes as input a string and n_buckets and assigns the string to one of the buckets.\n",
    "# This function is the bottle neck and takes forever to run\n",
    "string=$1\n",
    "n_buckets=$2\n",
    "\n",
    "# Convert to checksum then take the mod of it\n",
    "echo -n $string | cksum | cut -f1 -d\" \" | awk -v n=\"$n_buckets\" '{print $1\"%\"n}' | bc\n",
    "}\n",
    "\n",
    "sorted_file_prefix=sorted_pwordcount\n",
    "\n",
    "rm -f $sorted_file_prefix* *output\n",
    "\n",
    "# For each file with counts, create coallated files\n",
    "for file in $data.chunk.*.counts\n",
    "    do\n",
    "        cat $file | while read line\n",
    "            do\n",
    "                key=$(echo $line | cut -f1 -d\" \")\n",
    "                hash_key=$(hash_to_bucket $key $m)\n",
    "                echo $line >> $sorted_file_prefix.$hash_key\n",
    "            done\n",
    "    done\n",
    "\n",
    "# Sort these files\n",
    "for file in $sorted_file_prefix*\n",
    "    do\n",
    "        sort $file -o $file\n",
    "    done\n",
    "#\n",
    "###----------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $sorted_file_prefix* | perl -pe 's/\\n/ /'` # replace file reference to one from the sort.\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "for file in $sorted_file_prefix*\n",
    "    do\n",
    "        <$file ./reducer.py >> $data.output\n",
    "    done\n",
    "    \n",
    "sort $data.output -o $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the execution priviledges to make the shell script executable by all\n",
    "!chmod a+x pWordCount.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please feel free to adopt and modify the following mapper  for your purpose¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "findword = sys.argv[1]\n",
    "findword = findword.lower()\n",
    "\n",
    "if findword != \"*\":\n",
    "    findword = set(findword.split())\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Turn the list into a collection of lowercase words\n",
    "    for word in re.findall(r'[a-z]+', line.lower()):\n",
    "        if (word in findword) or (findword == \"*\"):\n",
    "            print(\"%s 1\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please feel free to adopt and modify the following reducer for your purpose¶\n",
    "\n",
    "####  (i.e., there will be no need for a sort in reducer.py code; leverage mapreduce framework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "total = 1\n",
    "current_word = None\n",
    "\n",
    "for countStr in sys.stdin:\n",
    "    word, count = countStr.split()\n",
    "    if current_word == word:\n",
    "        total += int(count)\n",
    "    elif current_word is None:\n",
    "        current_word = word\n",
    "    else:\n",
    "        print(current_word, total)\n",
    "        current_word = word\n",
    "        total = 1\n",
    "print(current_word, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dont forget to add a sort component to your MapReduce framework and leverage the sort order in your reduceer (i.e., there will be no need for a sort in reducer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e., insert code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: No arguments supplied\r\n",
      "\r\n",
      "To run use\r\n",
      "     pWordCount.sh m wordlist inputFile\r\n",
      "Input:\r\n",
      "      number of processes/maps, EG, 4\r\n",
      "      wordlist = a space-separated list of words in quotes, e.g., 'the and of'\r\n",
      "      inputFile = a text input file\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x pWordCount.sh\n",
    "! ./pWordCount.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW1.3 WordCount via Command Line Map Reduce Framework  <a name=\"1.3\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Write the mapper.py/reducer.py combination to perform WordCount using the command line mapreeduce framework containing an alphabetical listing of all the words, and the number of times each occurs, in the text version of Alice’s Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org/cache/epub/11/pg11.txt) The first 10 lines of your output file should look something like this (the counts are not totally precise):\n",
    "\n",
    "To do so, make sure of the following:\n",
    "   \n",
    "* That the mapper.py counts all occurrences of a single word\n",
    "* In the pWordCount.sh, please insert a sort command between the mappers (after the for loop) and the reducer calls to collate the output key-value pair records by key from the mappers. E.g., sort -k1,1. Use \"man sort\" to learn more about Unix sorts.\n",
    "* reducer.py sums the count value from the collated records for each  word. There should be no sort in the reducer.py\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word              Count\n",
    "=======================\n",
    "a                 631\n",
    "a-piece           1\n",
    "abide             1\n",
    "able              1\n",
    "about             94\n",
    "above             3\n",
    "absence           1\n",
    "absurd            2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, mapper.py will read in a portion (i.e., a single record corresponding to a row) of the email data,\n",
    "count the number of occurences of the  word in questions and print/emit a count to the output stream. While the utility of the reducer responsible for reading in counts of the word and summarizing them before printing that summary to the output stream.\n",
    "See example the [notebook](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/5zq0faibmvtjlbr/DivideAndConquer2-python-Plus-CmdLine.ipynb)\n",
    "See video section 1.12.1 1.12.1 Poor Man's MapReduce Using Command Line (Part 2) located at: \n",
    "https://learn.datascience.berkeley.edu/mod/page/view.php?id=10961\n",
    "\n",
    "NOTE in your python notebook create a cell to save your mapper/reducer to disk using magic commands (see example here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! ./pWordCount.sh 4 \"*\" alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 690)\r\n",
      "('abide', 2)\r\n",
      "('able', 1)\r\n",
      "('about', 102)\r\n",
      "('above', 3)\r\n",
      "('absence', 1)\r\n",
      "('absurd', 2)\r\n",
      "('accept', 1)\r\n",
      "('acceptance', 1)\r\n",
      "('accepted', 2)\r\n"
     ]
    }
   ],
   "source": [
    "!head alice.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW1.4  <a name=\"1.4\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Change the mapper.py/reducer.py combination so that you get only the number of words starting with an uppercase letter, and the number of words starting with a lowercase letter for Alice in Wonderland available [here](http://www.gutenberg.org/cache/epub/11/pg11.txt). In other words, you need an output file with only 2 lines, one giving you the number of words staring with a lowercase ('a' to 'z'), and the other line indicating the number of words starting with an uppercase letter ('A' to 'Z'). In the pWordCount.sh, please insert a sort command between the mappers (after the for loop) and the reducer calls to collate the output key-value pair records by key from the mappers. E.g., sort -k1,1. Use \"man sort\" to learn more about Unix sorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Turn the list into a collection of lowercase words\n",
    "    for word in re.findall(r'[a-zA-Z]+', line):\n",
    "        if word.isupper():\n",
    "            print(\"Capital 1\")\n",
    "        else:\n",
    "            print(\"Lower 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! ./pWordCount.sh 4 \"*\" alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Capital', 1096)\r\n",
      "('Lower', 29323)\r\n"
     ]
    }
   ],
   "source": [
    "! cat alice.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW1.5 Bias-Variance (This is an OPTIONAL HW) <a name=\"1.5\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Provide and example of bias variance in action for a similated function y = f(x). E.g., y = sin(x+x^2). Provide code, data, and graphs. \n",
    "\n",
    "Using a bias-variance decomposition analsysis on your choosen problem, describe how you would decide which model to choose when you dont know the true function and how does this choice compares to the choice you made using the true function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#TOC)\n",
    "<center><div class='jumbotron'><h2 style='color:green'>-------  END OF HOWEWORK --------</h2></div></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
